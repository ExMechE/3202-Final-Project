{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xmrLGa4AoKB7"
      },
      "outputs": [],
      "source": [
        "#pip install swig\n",
        "#pip install gym[box2d]\n",
        "#pip install keras-rl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import torch as torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as tnn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "IAEaSRZbqfyI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, learning_rate, state_dimensions, fc1_dims, fc2_dims, num_actions):\n",
        "        super(QNetwork, self).__init__()\n",
        "\n",
        "        self.state_dimensions = state_dimensions\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # Define the neural network layers\n",
        "        self.fc1_layer = nn.Linear(*self.state_dimensions, self.fc1_dims)\n",
        "        self.fc2_layer = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "        self.fc3_layer = nn.Linear(self.fc2_dims, self.num_actions)\n",
        "\n",
        "        # Optimizer/loss function\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        self.loss_function = nn.MSELoss()\n",
        "\n",
        "        # Device specification\n",
        "        self.device = 'cpu'\n",
        "        self.to(self.device)\n",
        "\n",
        "    def fwd(self, state):\n",
        "        # Forward pass through the network\n",
        "        x = tnn.relu(self.fc1_layer(state))\n",
        "        x = tnn.relu(self.fc2_layer(x))\n",
        "        actions = self.fc3_layer(x)\n",
        "\n",
        "        return actions"
      ],
      "metadata": {
        "id": "OGT4HDYhjv2Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReinforcementLearningAgent():\n",
        "    def __init__(self, discount_factor, exploration_rate, learning_rate, state_dimensions, batch_size, num_actions,\n",
        "                 max_memory_size=100000, exploration_end=0.01, exploration_decay=5e-4, q_network=None):\n",
        "\n",
        "        # Initialize the reinforcement learning agent\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.learning_rate = learning_rate\n",
        "        self.state_dimensions = state_dimensions\n",
        "        self.action_space = [i for i in range(num_actions)]\n",
        "        self.max_memory_size = max_memory_size\n",
        "        self.batch_size = batch_size\n",
        "        self.memory_counter = 0\n",
        "        self.exploration_end = exploration_end\n",
        "        self.exploration_decay = exploration_decay\n",
        "\n",
        "        # Q-network for evaluation\n",
        "        self.q_network = q_network if q_network else QNetwork(\n",
        "            self.learning_rate, num_actions=num_actions, state_dimensions=state_dimensions, fc1_dims=256, fc2_dims=256\n",
        "        )\n",
        "\n",
        "        # Replay memory\n",
        "        self.state_memory = np.zeros((self.max_memory_size, *state_dimensions), dtype=np.float32)\n",
        "        self.new_state_memory = np.zeros((self.max_memory_size, *state_dimensions), dtype=np.float32)\n",
        "        self.action_memory = np.zeros(self.max_memory_size, dtype=np.int32)\n",
        "        self.reward_memory = np.zeros(self.max_memory_size, dtype=np.float32)\n",
        "        self.terminal_memory = np.zeros(self.max_memory_size, dtype=bool)\n",
        "\n",
        "    def store_experience(self, current_state, action, reward, new_state, is_terminal):\n",
        "        # Store a new experience in the replay memory\n",
        "        index = self.memory_counter % self.max_memory_size\n",
        "        self.state_memory[index] = current_state\n",
        "        self.new_state_memory[index] = new_state\n",
        "        self.reward_memory[index] = reward\n",
        "        self.action_memory[index] = action\n",
        "        self.terminal_memory[index] = is_terminal\n",
        "        self.memory_counter += 1\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        # Choose an action based on epsilon-greedy strategy\n",
        "        random_value = np.random.random()\n",
        "        if random_value > self.exploration_rate:\n",
        "            current_state = torch.tensor([observation]).to(self.q_network.device)\n",
        "            actions = self.q_network.fwd(current_state)\n",
        "            chosen_action = torch.argmax(actions).item()\n",
        "        else:\n",
        "            chosen_action = np.random.choice(self.action_space)\n",
        "        return chosen_action\n",
        "\n",
        "    def update_q_network(self):\n",
        "        # Update the Q-network using a batch of experiences from the replay memory\n",
        "        if self.memory_counter < self.batch_size:\n",
        "            return\n",
        "\n",
        "        self.q_network.optimizer.zero_grad()\n",
        "\n",
        "        max_memory = min(self.memory_counter, self.max_memory_size)\n",
        "        batch_indices = np.random.choice(max_memory, self.batch_size, replace=False)\n",
        "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
        "\n",
        "        current_state_batch = torch.tensor(self.state_memory[batch_indices]).to(self.q_network.device)\n",
        "        new_state_batch = torch.tensor(self.new_state_memory[batch_indices]).to(self.q_network.device)\n",
        "        reward_batch = torch.tensor(self.reward_memory[batch_indices]).to(self.q_network.device)\n",
        "        terminal_batch = torch.tensor(self.terminal_memory[batch_indices]).to(self.q_network.device)\n",
        "        action_batch = self.action_memory[batch_indices]\n",
        "\n",
        "        q_values_current_state = self.q_network.fwd(current_state_batch)[batch_index, action_batch]\n",
        "        q_values_new_state = self.q_network.fwd(new_state_batch)\n",
        "        q_values_new_state[terminal_batch] = 0.0\n",
        "\n",
        "        q_target = reward_batch + self.discount_factor * torch.max(q_values_new_state, dim=1)[0]\n",
        "\n",
        "        loss = self.q_network.loss_function(q_target, q_values_current_state).to(self.q_network.device)\n",
        "        loss.backward()\n",
        "        self.q_network.optimizer.step()\n",
        "\n",
        "        self.exploration_rate = max(self.exploration_rate - self.exploration_decay, self.exploration_end)"
      ],
      "metadata": {
        "id": "lUtPNKKvW9vz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.wrappers import RecordVideo\n",
        "def train_lunar_lander_agent():\n",
        "\n",
        "    # Create the LunarLander environment\n",
        "    lunar_lander_env = gym.make(\"LunarLander-v2\", render_mode='rgb_array')\n",
        "    lunar_lander_env = RecordVideo(lunar_lander_env, 'video')  # Assuming RecordVideo is a custom wrapper for video recording\n",
        "\n",
        "    # Initialize the reinforcement learning agent\n",
        "    landing_agent = ReinforcementLearningAgent(\n",
        "        discount_factor = 0.99, exploration_rate=1.0, learning_rate=0.0002, state_dimensions=[8],\n",
        "        num_actions=4, max_memory_size = 1000000, batch_size = 64, exploration_end = 0.04, exploration_decay=0.8\n",
        "    )\n",
        "\n",
        "    episode_scores, epsilon_history = [], []\n",
        "    num_episodes = 300\n",
        "\n",
        "    # Run training for a specified number of episodes\n",
        "    for episode in range(num_episodes):\n",
        "        total_score = 0\n",
        "        episode_done = False\n",
        "        current_observation = lunar_lander_env.reset(seed=42)\n",
        "\n",
        "        while not episode_done:\n",
        "            # Choose an action based on the agent's policy\n",
        "            selected_action = landing_agent.choose_action(current_observation)\n",
        "\n",
        "            # Take the chosen action and observe the new state and reward\n",
        "            new_observation, reward, episode_terminated, truncated = lunar_lander_env.step(selected_action)\n",
        "            total_score += reward\n",
        "\n",
        "            # Check if the episode is done (either terminated or truncated)\n",
        "            episode_done = episode_terminated or truncated\n",
        "\n",
        "            # Store the transition in the agent's replay memory and update the Q-network\n",
        "            landing_agent.store_experience(current_observation, selected_action, reward, new_observation, episode_done)\n",
        "            landing_agent.update_q_network()\n",
        "\n",
        "            current_observation = new_observation\n",
        "\n",
        "        # Record scores and epsilon values for analysis\n",
        "        episode_scores.append(total_score)\n",
        "        epsilon_history.append(landing_agent.exploration_rate)\n",
        "        print('Episode score: ', total_score)\n",
        "\n",
        "    lunar_lander_env.close()"
      ],
      "metadata": {
        "id": "mv7251GMk2HP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.10/dist-packages')\n",
        "import gym\n",
        "train_lunar_lander_agent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrY9Ff2Yk8Ew",
        "outputId": "fec559d7-7a24-4156-b2cb-962961fc3f72"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/record_video.py:78: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "<ipython-input-4-76c5cdbe44a0>:43: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  current_state = torch.tensor([observation]).to(self.q_network.device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode score:  -641.5298915229102\n",
            "Episode score:  -230.1482760617113\n",
            "Episode score:  -682.1084742371035\n",
            "Episode score:  -494.2570911835446\n",
            "Episode score:  -105.1321215580107\n",
            "Episode score:  -119.25314439205735\n",
            "Episode score:  -145.74621382358728\n",
            "Episode score:  -143.66854751800926\n",
            "Episode score:  -324.0776087396532\n",
            "Episode score:  -562.1780806885058\n",
            "Episode score:  -240.6936461764117\n",
            "Episode score:  -312.1204966813717\n",
            "Episode score:  -100.17789720392433\n",
            "Episode score:  194.63070617397045\n",
            "Episode score:  -250.77126901330712\n",
            "Episode score:  -338.9177413944261\n",
            "Episode score:  -259.84674415650943\n",
            "Episode score:  -86.83894954272257\n",
            "Episode score:  -97.02410438017736\n",
            "Episode score:  -316.6907383668686\n",
            "Episode score:  -245.65779633941656\n",
            "Episode score:  -172.43830679931364\n",
            "Episode score:  -386.14951886222093\n",
            "Episode score:  -155.81112773293557\n",
            "Episode score:  -153.55647401985516\n",
            "Episode score:  -135.30200720334702\n",
            "Episode score:  -144.74100641629175\n",
            "Episode score:  -135.89541997611292\n",
            "Episode score:  -396.2852594306313\n",
            "Episode score:  -355.8930279443317\n",
            "Episode score:  -151.7272133924082\n",
            "Episode score:  -124.80749821452474\n",
            "Episode score:  -113.2235955379159\n",
            "Episode score:  -143.70525550259862\n",
            "Episode score:  -92.65827079614363\n",
            "Episode score:  -105.09939134715506\n",
            "Episode score:  -161.55598136541977\n",
            "Episode score:  -94.63795805073083\n",
            "Episode score:  -118.73605022605452\n",
            "Episode score:  -137.0209858401426\n",
            "Episode score:  -112.96833379570384\n",
            "Episode score:  -151.66037135434937\n",
            "Episode score:  -369.61212543976177\n",
            "Episode score:  -167.83669248923397\n",
            "Episode score:  179.8223245734195\n",
            "Episode score:  -278.99822250163925\n",
            "Episode score:  -250.7934404942109\n",
            "Episode score:  -268.76815418301027\n",
            "Episode score:  -225.73788449572447\n",
            "Episode score:  -242.42998154383432\n",
            "Episode score:  -251.26827856748\n",
            "Episode score:  -85.05555665124915\n",
            "Episode score:  -81.07758061661468\n",
            "Episode score:  -145.3891295216631\n",
            "Episode score:  208.93952985959504\n",
            "Episode score:  -232.6314705274004\n",
            "Episode score:  -54.70899333149928\n",
            "Episode score:  -64.32210759850274\n",
            "Episode score:  -96.60034146335003\n",
            "Episode score:  -54.015401483335054\n",
            "Episode score:  161.28944891102097\n",
            "Episode score:  -219.9903545872577\n",
            "Episode score:  -200.7707221520887\n",
            "Episode score:  -123.06377856424565\n",
            "Episode score:  -43.04645184335668\n",
            "Episode score:  -83.81868601401979\n",
            "Episode score:  -15.26971517567884\n",
            "Episode score:  -216.41216062006444\n",
            "Episode score:  -87.07098055766284\n",
            "Episode score:  -225.1465262467708\n",
            "Episode score:  -67.25544409493202\n",
            "Episode score:  -42.99203218311601\n",
            "Episode score:  -120.62410229085836\n",
            "Episode score:  -142.17385488728746\n",
            "Episode score:  -211.67117266042078\n",
            "Episode score:  -157.0334142603765\n",
            "Episode score:  -198.51132666175948\n",
            "Episode score:  34.73786808690767\n",
            "Episode score:  -227.18982996338286\n",
            "Episode score:  -71.53873154939171\n",
            "Episode score:  -88.59623101676112\n",
            "Episode score:  -199.01797908130413\n",
            "Episode score:  -91.85469962770898\n",
            "Episode score:  -112.83957870151896\n",
            "Episode score:  -35.71164479368652\n",
            "Episode score:  -108.62319966355302\n",
            "Episode score:  -98.8224695372678\n",
            "Episode score:  -256.35767820325225\n",
            "Episode score:  172.39998304995567\n",
            "Episode score:  -104.73548044274445\n",
            "Episode score:  -136.7810010884501\n",
            "Episode score:  -101.1614521911991\n",
            "Episode score:  -94.15406470232247\n",
            "Episode score:  190.09090019748538\n",
            "Episode score:  -68.7556865566084\n",
            "Episode score:  -94.60333172145255\n",
            "Episode score:  -206.1499130300109\n",
            "Episode score:  -87.96090989560784\n",
            "Episode score:  -81.50438073607329\n",
            "Episode score:  -23.00574604980686\n",
            "Episode score:  -59.74739919468588\n",
            "Episode score:  -56.55932117104436\n",
            "Episode score:  -90.27058456416611\n",
            "Episode score:  -308.1570611047928\n",
            "Episode score:  -304.65704965977494\n",
            "Episode score:  -86.02227649592214\n",
            "Episode score:  -105.90437121306036\n",
            "Episode score:  -287.51365662655\n",
            "Episode score:  -280.6081762887352\n",
            "Episode score:  -229.98739148137275\n",
            "Episode score:  -91.19347651503504\n",
            "Episode score:  -48.16535247658981\n",
            "Episode score:  -26.308712983659575\n",
            "Episode score:  -230.54522904971438\n",
            "Episode score:  205.10782347578444\n",
            "Episode score:  -291.01005793641616\n",
            "Episode score:  -281.37111551929536\n",
            "Episode score:  -71.58338094132657\n",
            "Episode score:  -82.96338189395615\n",
            "Episode score:  -30.42024867996797\n",
            "Episode score:  -132.40044718618623\n",
            "Episode score:  -134.4367467814655\n",
            "Episode score:  -130.57692780355748\n",
            "Episode score:  -230.82789622261538\n",
            "Episode score:  -50.94201876249371\n",
            "Episode score:  -174.9363214336882\n",
            "Episode score:  -108.65581486333386\n",
            "Episode score:  -122.51220275167775\n",
            "Episode score:  -129.242929705093\n",
            "Episode score:  -143.97610069801604\n",
            "Episode score:  -139.23474540337878\n",
            "Episode score:  -127.50426380560285\n",
            "Episode score:  -130.45024013609304\n",
            "Episode score:  -123.23053504083393\n",
            "Episode score:  -132.9554112279765\n",
            "Episode score:  -121.11474100438011\n",
            "Episode score:  -129.7575573962445\n",
            "Episode score:  -131.5237628558618\n",
            "Episode score:  -126.56143419883574\n",
            "Episode score:  -114.14028645102161\n",
            "Episode score:  -123.38858177923282\n",
            "Episode score:  -118.54827439558886\n",
            "Episode score:  -121.50659925852254\n",
            "Episode score:  -111.86667394435278\n",
            "Episode score:  -125.6337498916686\n",
            "Episode score:  -111.86974138022853\n",
            "Episode score:  -118.31159475084398\n",
            "Episode score:  -112.60924370539612\n",
            "Episode score:  -116.59152230475159\n",
            "Episode score:  -119.68104501016676\n",
            "Episode score:  -125.14488156088322\n",
            "Episode score:  -114.07397214041895\n",
            "Episode score:  -105.00460254307748\n",
            "Episode score:  -119.45890743447674\n",
            "Episode score:  -108.86691829300166\n",
            "Episode score:  -110.11487400640104\n",
            "Episode score:  -111.67244802782194\n",
            "Episode score:  -111.98443909584415\n",
            "Episode score:  -112.2410750461033\n",
            "Episode score:  -109.54014654467842\n",
            "Episode score:  -106.33582371862009\n",
            "Episode score:  -113.57651525610791\n",
            "Episode score:  -106.9490880992012\n",
            "Episode score:  -103.8148930226896\n",
            "Episode score:  -118.83407952204566\n",
            "Episode score:  -104.87960924363561\n",
            "Episode score:  -102.74606695251761\n",
            "Episode score:  -110.61613894162404\n",
            "Episode score:  -99.1366342812403\n",
            "Episode score:  -102.03379528216036\n",
            "Episode score:  -106.04035876575313\n",
            "Episode score:  -98.3020761436406\n",
            "Episode score:  -105.81371244405939\n",
            "Episode score:  -96.79314611390741\n",
            "Episode score:  -90.65464090979088\n",
            "Episode score:  -93.32112564821028\n",
            "Episode score:  -88.93083458777483\n",
            "Episode score:  -99.4019361259831\n",
            "Episode score:  -87.77039728767882\n",
            "Episode score:  -96.05533334374901\n",
            "Episode score:  -92.74715853026919\n",
            "Episode score:  -103.45612070458002\n",
            "Episode score:  -92.05024492953319\n",
            "Episode score:  -93.00480715835873\n",
            "Episode score:  -80.56817082937218\n",
            "Episode score:  -90.44148598778071\n",
            "Episode score:  -89.04654074975002\n",
            "Episode score:  -82.72420413977842\n",
            "Episode score:  -77.15644975093117\n",
            "Episode score:  -106.5547363048292\n",
            "Episode score:  -120.60911099587543\n",
            "Episode score:  -91.80651678200638\n",
            "Episode score:  -293.99788259137625\n",
            "Episode score:  -76.43846197841843\n",
            "Episode score:  -73.37824725643719\n",
            "Episode score:  -77.20687084393606\n",
            "Episode score:  -69.84824773921876\n",
            "Episode score:  -77.27565596073568\n",
            "Episode score:  -78.20202814753789\n",
            "Episode score:  -79.2278087751188\n",
            "Episode score:  34.194454475438526\n",
            "Episode score:  -81.58897774230263\n",
            "Episode score:  -93.08466652851305\n",
            "Episode score:  -203.87487121211515\n",
            "Episode score:  -99.6360329904867\n",
            "Episode score:  -33.1124592510059\n",
            "Episode score:  -67.18239773721334\n",
            "Episode score:  -70.7212045118733\n",
            "Episode score:  -76.60918902738595\n",
            "Episode score:  -64.08646866575084\n",
            "Episode score:  -69.18115572441958\n",
            "Episode score:  -76.92410523237795\n",
            "Episode score:  -61.72243959990699\n",
            "Episode score:  -68.4423954819628\n",
            "Episode score:  -72.5058940376009\n",
            "Episode score:  -65.72503740525426\n",
            "Episode score:  -67.33981499120185\n",
            "Episode score:  -67.85687739514259\n",
            "Episode score:  -64.88173738941126\n",
            "Episode score:  -69.802823625505\n",
            "Episode score:  -71.7948724206588\n",
            "Episode score:  -76.5161193663888\n",
            "Episode score:  82.87119354333254\n",
            "Episode score:  -61.13755966288195\n",
            "Episode score:  -67.13407268203622\n",
            "Episode score:  -66.50650578513131\n",
            "Episode score:  -65.28958799251413\n",
            "Episode score:  -89.51799191330116\n",
            "Episode score:  -84.17514515859155\n",
            "Episode score:  -63.008305300535426\n",
            "Episode score:  -75.47338522687679\n",
            "Episode score:  -69.48095469452211\n",
            "Episode score:  -65.32482559464731\n",
            "Episode score:  -72.07523880885215\n",
            "Episode score:  -77.07476264124021\n",
            "Episode score:  97.09073103545167\n",
            "Episode score:  -82.71502092367149\n",
            "Episode score:  51.16177010211217\n",
            "Episode score:  -54.50095068950037\n",
            "Episode score:  -71.27011360466345\n",
            "Episode score:  -71.5942580710631\n",
            "Episode score:  -80.99374175061887\n",
            "Episode score:  53.21040266948911\n",
            "Episode score:  -28.849568757964015\n",
            "Episode score:  69.50478179051316\n",
            "Episode score:  76.62064804911357\n",
            "Episode score:  83.95087730805307\n",
            "Episode score:  85.22386373778876\n",
            "Episode score:  78.57158698147317\n",
            "Episode score:  85.965495998776\n",
            "Episode score:  79.18230097923765\n",
            "Episode score:  85.51975937493758\n",
            "Episode score:  117.91909258059817\n",
            "Episode score:  -64.8840782946179\n",
            "Episode score:  136.21023560709227\n",
            "Episode score:  -59.7618062097675\n",
            "Episode score:  -64.98332096329626\n",
            "Episode score:  75.16258866649757\n",
            "Episode score:  -59.60591044747859\n",
            "Episode score:  -61.58428077546701\n",
            "Episode score:  81.47583868533461\n",
            "Episode score:  -65.65433850122268\n",
            "Episode score:  -68.3124240361963\n",
            "Episode score:  -56.40651212862862\n",
            "Episode score:  -63.74153781582262\n",
            "Episode score:  -35.56727056030631\n",
            "Episode score:  -70.78954087662727\n",
            "Episode score:  -65.39109588243436\n",
            "Episode score:  -79.7416414679332\n",
            "Episode score:  -103.6128723913966\n",
            "Episode score:  -54.58316129647575\n",
            "Episode score:  77.68314738835451\n",
            "Episode score:  -63.70587663637555\n",
            "Episode score:  -65.96829028582756\n",
            "Episode score:  -59.738019660676215\n",
            "Episode score:  -60.01743668179041\n",
            "Episode score:  -70.42524308589117\n",
            "Episode score:  -70.41365145383965\n",
            "Episode score:  -67.89267466735988\n",
            "Episode score:  -50.99831729291677\n",
            "Episode score:  103.49331443550405\n",
            "Episode score:  125.33472456217528\n",
            "Episode score:  -37.91537715335242\n",
            "Episode score:  132.3828679458369\n",
            "Episode score:  -53.16136849803527\n",
            "Episode score:  114.31447000767838\n",
            "Episode score:  122.10861559465287\n",
            "Episode score:  149.02815795916732\n",
            "Episode score:  119.4528379520377\n",
            "Episode score:  107.03315941189494\n",
            "Episode score:  120.68427486087805\n",
            "Episode score:  116.40918656007119\n",
            "Episode score:  103.78147218559968\n",
            "Episode score:  -70.3512550963804\n",
            "Episode score:  126.75572023641134\n",
            "Episode score:  145.33982225657445\n",
            "Episode score:  -42.812860462168985\n",
            "Episode score:  121.19718924497883\n",
            "Episode score:  106.50280385782983\n",
            "Episode score:  108.8526527424195\n"
          ]
        }
      ]
    }
  ]
}